<!-- Template for PROJECT REPORT of CapstoneDesign 2025-2H, initially written by khyoo -->
<!-- 본 파일은 2025년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->

# Team-Info
| (1) 과제명 | *홀로 K-pop 안무를 배우는 사용자를 대상으로 Mediapipe 동작 인식과 SAM2 안무 시각화, 3D 기술을 활용한 어려움 해결 및 지속적인 동기부여 제공 서비스*
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | 39-캡짱 |
| (3) 팀 구성원 | - 김유민 (2171007): 리더, *SAM2와 OpenCV를 활용한 안무 실루엣 마스킹 및 컨투어 추출. Spring Boot 기반 백엔드 REST API 설계 및 구현. MySQL을 활용한 데이터베이스 설계 및 관리. AWS 기반 서버 운영 및 배포.* <br><br> - 남궁도윤 (2276098): 팀원, *Mediapipe와 OpenCV를 활용한 안무 동작 분석 및 평가/피드백 시스템 개발 및 검증. MySQL 기반 데이터베이스 모델링 및 구조 설계/유지보수. Spring Boot 백엔드 개발 및 REST API 설계/구현.* <br><br> - 정소희 (2276282) : 팀원, *Unity를 이용한 아바타 기능 구현. Android Studio를 이용한 UI 및 기능 구현.*			 |
| (4) 팀 지도교수 | 이형준 교수님 |
| (5) 과제 분류 | 산학과제 |
| (6) 과제 키워드 | 춤 연습, 안무 시각화, 동작 비교  |
| (7) 과제 내용 요약 |  저희 과제는 K-pop 안무를 혼자 연습하는 사용자의 시간·비용 부담, 학습 효율 및 동기 저하, 재미 부족 문제를 해결하기 위해 기획되었습니다. <br> 실시간 사용자의 동작을 분석을 위해 MediaPipe를 활용하고, 부족한 동작에 대해서는 Gemini를 통해 사용자에게 맞춤형 피드백을 제공합니다. 안무가의 실루엣을 시각화하기 위해  SAM2와 OpenCV를 사용합니다. 또한 사용자의 동기부여를 강화를 위해 Unity 기반 가상 아이돌과의 챌린지 모드를 구현하였습니다. <br> 이러한 기술들을 사용하여 안무 유사도 평가, AI 기반 맞춤형 피드백, 네온 효과를 입힌 안무가 실루엣 제공 등을 통해 더욱 직관적인 학습 환경을 제공하고자 합니다. 따라서 사용자는 장소나 시간 제약 없이 편리하게 반복적인 연습이 가능하며, 지속적인 동기부여와 피드백을 통한 실력 향상을 기대할 수 있습니다. |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 |  **Target Customer** <br><li>K-pop 안무를 배우고 싶은 국내외 팬 <br><li> 시간이 부족한 학생 및 직장인<br><li> 오프라인 학원 수강이 어려운 사용자 (경제적 부담 등)<br><li> 혼자 춤을 배우는데 동기부여나 피드백이 부족한 사용자 <br><br>**Pain Points**<br><li>  문제 1: 시간과 비용의 제약<br>댄스 학원에 다니고 싶어도 시간적, 경제적 이유로 부담을 느끼는 사람들이 많습니다. <br><li> 문제 2: 학습 동작 확인의 어려움<br>유튜브로 안무를 학습할 때 거울이 없으면 자신의 동작이 올바른지 한눈에 확인하기 어렵습니다. 또한 안무가의 영상과 자신의 안무를 한번에 확인할 수 없습니다.<br><li>  문제 3: 혼자 연습할 때의 지루함 <br>혼자 춤을 연습하는 경우 반복적인 안무 학습으로 인해 쉽게 지루해질 수 있습니다. 이를 보완하기 위해 가상 아바타와 함께 안무를 수행하는 챌린지 모드를 도입하여 혼자 연습하는 데서 오는 단조로움을 완화할 수 있습니다.  |
| (2) 기존연구와의 비교 | 유사한 서비스로는 K-pop 안무 학습 앱 Stream이 있으며, 저희 서비스와 비교해보았습니다.<br><br><li> **공통점**: 사용자가 안무 가이드를 따라, 혼자서도 안무를 학습할 수 있도록 구성되어 있습니다. <br><br><li> **기존 서비스의 한계**: Stream은 안무가 위에 실루엣이 덧입혀져 있어 세부 동작을 따라하거나, 안무의 정확한 포인트를 파악하는 데 어려움이 있을 수 있습니다. <br><br><li> **우리 서비스의 장점**:<br> 1. 기존 안무의 실루엣 자체를 영상에서 분리해 사용자에게 직접 제공함으로써, 사용자가 정확한 동작 라인을 명확히 인식하고 따라할 수 있도록 설계되었습니다. <br> 2. 기존 안무 실루엣에 네온 효과를 적용해 동작을 강조함으로써, 직관적인 시각적 가이드를 제공합니다. <br> 3. ‘정확도 모드’에서 사용자의 실시간 동작을 분석한 후, 전문가의 안무와의 유사도를 정량적으로 계산해 실시간으로 피드백을 제공합니다. 또한 정확도 모드가 끝난 이후, 점수가 낮은 구간을 조회해 사용자 맞춤 코칭을 제공합니다. <br> 이를 통해 사용자는 자신의 동작 정확도를 즉시 확인하고 개선할 수 있는 구체적인 방향을 얻을 수 있습니다. <br> 4. ‘챌린지 모드’에서는 Unity 기반으로 생성된 가상의 아이돌 아바타와 함께 춤을 즐길 수 있으며, 이는 사용자의 몰입도와 흥미를 유발합니다. <br><br><li> **우리 서비스의 단점**: 한정적으로 제공된 K-pop 안무 영상 기반으로 학습이 이루어지며, Stream 서비스와 달리 사용자가 원하는 영상을 업로드하는 기능은 포함되어 있지 않습니다. <br><br><li> **우리 서비스의 발전 가능성**: 향후 다양한 음악 장르를 지원하기 위해, 사용자가 직접 원하는 영상을 업로드할 수 있는 기능도 고려하고 있습니다. |
| (3) 제안 내용 | 먼저 사용자가 시간과 장소에 구애받지 않고 자유롭게 연습할 수 있도록, 모바일 기반의 안무 연습 서비스를 제공합니다. <br> 또한 사용자가 학습 동작을 확인할 때의 어려움을 해결하기 위해서 MediaPipe 활용하여 사용자의 동작을 실시간으로 분석하고, 안무가의 동작과의 유사도를 수치화하여 피드백을 제공합니다. 정확도 모드 종료 후에는 점수가 낮았던 구간을 기반으로 Gemini를 활용해 맞춤형 1줄 피드백을 생성하여 사용자에게 보다 구체적인 개선 방향을 제시합니다. 추가적으로 안무 실루엣 기능을 SAM2를 사용하여 제공함으로써 사용자가 동작을 더 정확하게 따라할 수 있도록 도와줍니다. <br> 마지막으로 사용자가 혼자 연습할 때 느끼는 지루함을 줄이기 위해, Unity 기반의 가상 아이돌 아바타와 함께하는 챌린지 모드를 도입하였습니다. <br> 이처럼 저희 서비스는 사용자가 언제 어디서든 자유롭게 안무를 연습하고 실시간 분석과 AI 기반 맞춤형 피드백을 통해 자신의 동작을 개선할 수 있도록 돕습니다. 또한 재미있고 꾸준히 연습할 수 있는 환경을 제공함으로써 사용자 스스로 실력을 향상시킬 수 있도록 지원합니다. |
| (4) 기대효과 및 의의 | **1. 편리한 연습 환경**<br><li> 시간과 장소에 구애받지 않고 자유롭게 춤을 연습할 수 있으며, 수강료를 걱정하지 않아도 됩니다. <br><br> **2. 개인 실력 향상** <br><li> 피드백을 통해 사용자의 춤 실력 향상에 도움을 줍니다. <br><br> **3. 지루함 완화** <br><li> 혼자 춤을 출 때 느낄 수 있는 지루함과 단조로움을 가상 아이돌 챌린지를 통해 완화할 수 있습니다. <br><br> **4. 지속적인 동기부여** <br><li> 반복되는 연습 과정에서 오는 단순함을 덜어주고, 사용자가 연습을 꾸준히 이어갈 수 있도록 돕습니다. |
| (5) 주요 기능 리스트 | **1. 연습 모드**	<br><li>전체 안무 / 하이라이트 안무 선택 가능<br><li> 춤추는 화면에서 영상 재생 및 실루엣 가이드 제공 <br><li>재생 컨트롤: 일시정지, 속도 조절, 구간 반복 <br><br>**2. 정확도 검증 모드 (MediaPipe 기반)**<br><li>	전문가와의 포즈 유사도 비교 (관절 위치 및 각도 분석)	<br><li>정확도 점수 계산을 바탕으로 피드백 제공 (Excellent, Good, OK, Miss) <br><br>**3. 챌린지 모드 (Unity 기반)**	<br><li>3D 가상 아이돌 캐릭터와 함께 안무 수행<br><li>	아바타와 함께하는 댄스 챌린지 → 재미 요소 강화<br><br>**4. 분석 및 피드백 기능**<br><li>MediaPipe Pose 모델 기반 관절 좌표 추출<br><li>정확도 비교 알고리즘 (유클리드 거리, 각도 차이 등)<br><li>SAM2 기반 실루엣 마스크 추출<br>→ 네온 효과 시각 가독성 강화<br>-Gemini 기반 맞춤형 피드백 제공 (낮은 점수 구간 대상)<br><br>**5. 녹화 기능**<br><li>연습/정확도/챌린지 모드 녹화 및 저장|

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 |<h2>시스템 주요 기능 유스케이스</h2></br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/15156b6a737b6522ad238341eb82ac0d46d58dbe/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A4.png"  width="600" height="400"/></br><h2>1. 사용자 포즈 인식</h2><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/15156b6a737b6522ad238341eb82ac0d46d58dbe/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A41%EC%82%AC%EC%9A%A9%EC%9E%90%20%ED%8F%AC%EC%A6%88%20%EC%9D%B8%EC%8B%9D.png"  width="800" height="400"/></br>**■ 기능 개요**</br>사용자 포즈 인식은 사용자가 카메라 앞에서 수행하는 동작을 실시간으로 감지하고, 해당 영상 프레임을 Pose Estimation 엔진에 전달하여 관절 위치(x,y,z 좌표)를 추출하는 기능입니다. 이 과정은 일정 주기(1초마다)로 반복 수행되며, 실시간 피드백이나 정확도 평가의 기반 데이터로 사용됩니다.</br></br>**■ 구성 요소**</br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/f9da2d0c78b0ef2cb70fb6359df1dbd48b6ac975/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A41%EC%82%AC%EC%9A%A9%EC%9E%90%20%ED%8F%AC%EC%A6%88%20%EC%9D%B8%EC%8B%9D_%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C.png" width="400" height=180></br></br>**■ 시퀀스 흐름**</br>루프 구조: 실시간 반복 처리</br>전체 흐름은 일정 주기(1초)로 반복되는 구조이며,시퀀스 다이어그램 하단의 ⟳ 기호로 표현됩니다.</br>① 사용자 → 사용자 클라이언트 : 카메라 앞 동작 수행</br>- 사용자는 카메라 앞에서 안무를 수행하거나 포즈를 취함</br> - 이때 사용자 클라이언트는 카메라 스트림을 수신 중</br></br>② 사용자 클라이언트 → 자체 처리 (self-message): 사용자 프레임 캡처</br>- 실시간 스트림에서 특정 시점의 프레임을 이미지로 캡처</br>- 이 프레임은 1초 단위 간격으로 선택됨</br>- 1.1 사용자 프레임 캡처라는 self-message로 표현됨</br></br>③ 사용자 클라이언트 → 시스템 : POST /pose with frame</br>- 캡처된 프레임을 서버로 전송</br>- 보통 multipart/form-data로 이미지 데이터를 포함한 HTTP POST 요청</br>- 이 단계는 실시간 성능을 고려해 빠르게 처리되어야 하며, 네트워크 대역폭에 영향을 받음</br></br>④ 시스템 → 시스템 : 현재의 프레임 번호 계산</br>- 시스템은 요청이 들어온 시점을 기준으로 내부적으로 해당 프레임이 어떤 타임스탬프/번호에 해당하는지를 기록</br>- 이 정보는 이후 전문가 포즈와의 정렬(예: DTW)에서 중요함</br></br>⑤ 시스템 → MediaPipe 엔진 : pose 추정 요청</br>- 서버는 프레임 이미지를 MediaPipe 모델에 전달</br></br>⑥ MediaPipe → 시스템 : 관절 위치 추정 결과 반환</br>- MediaPipe는 33개 관절의 x, y, z 값을 포함한 JSON 반환</br>- 이 데이터는 이후 점수 계산에 사용됨</br></br>⑦ 시스템 → 사용자 클라이언트 : (선택) 결과 전달</br>- 다이어그램에는 표시되지 않았지만, 추후 사용자 사용자 클라이언트가 이 데이터를 시각화하거나 저장할 수 있음</br></br><h2>2. 전문가 안무 데이터 로딩</h2></br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/15156b6a737b6522ad238341eb82ac0d46d58dbe/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A42%EC%A0%84%EB%AC%B8%EA%B0%80%20%EC%95%88%EB%AC%B4%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%A1%9C%EB%94%A9.png" width="800" height="400"></br>**■ 기능 개요**</br>전문가 안무 데이터 로딩은 관리자가 업로드한 전문가 안무 영상으로부터 1초 단위 관절 데이터를 추출하고, 이를 JSON 형식으로 저장한 뒤 스토리지에 업로드하며, 이 JSON의 경로를 데이터베이스에 저장하는 일련의 과정을 포함합니다. 이 데이터는 추후 사용자의 포즈 비교, 정확도 평가에 사용됩니다.</br></br>**■ 구성 요소**</br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/f9da2d0c78b0ef2cb70fb6359df1dbd48b6ac975/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A42%EC%A0%84%EB%AC%B8%EA%B0%80%20%EC%95%88%EB%AC%B4%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%A1%9C%EB%94%A9_%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C.png" width="400" height=200></br></br>**■ 시퀀스 흐름**</br>① 관리자 → 시스템: 전문가 안무 영상 업로드</br>- 관리자는 비교용으로 사용할 전문가 안무 영상을 업로드</br>- 업로드는 웹 UI 또는 관리자 툴을 통해 수행</br></br>② 시스템 → MediaPipe 엔진: pose 추정 요청</br>- 시스템은 업로드된 영상을 내부적으로 1초마다 대표 프레임을 추출함</br>- 그 결과를 기반으로 MediaPipe 엔진에 pose 추정을 요청</br></br>③ MediaPipe 엔진: 1초 단위로 관절 위치 추정</br>- 각 프레임(1초마다 추출된 이미지)에서 전문가의 관절 위치를 추정</br>(33개 keypoint: 코, 어깨, 팔꿈치, 손목, 골반, 무릎 등)</br>- 결과는 (x,y,z)를 포함한 좌표 정보 리스트</br></br>④ MediaPipe → 시스템: 관절 데이터 반환</br>- MediaPipe는 추정한 관절 데이터를 시스템에 반환</br>- 보통 한 프레임당 1개의 keypoint 배열을 가지며, 다수의 프레임이 연결된 시퀀스 형태로 전달됨</br></br>⑤ 시스템 → 시스템 (내부 처리): 관절 데이터 전처리 및 JSON 생성</br>- 시스템은 전달받은 다수의 프레임별 keypoint 데이터를 정제, 구조화, 정규화함</br>- 이후 JSON 포맷으로 변환함 ({ frame_number: [...], ... })</br></br>⑥ 시스템 → 관리자: 관절 데이터 파일 저장</br>- JSON 파일로 저장된 관절 데이터를 정리한 뒤 파일을 저장함</br></br>⑦ 관리자→ Storage: 관절 데이터 JSON 파일 업로드</br>- JSON 파일을 외부 스토리지에 업로드 (AWS S3)</br>- 파일명은 {songId}_pose.json 등과 같이 고유하게 설정됨</br></br>⑧ 관리자→ DB: 업로드된 JSON의 스토리지 경로 저장</br>- 스토리지에서 반환된 URL 또는 경로를 DB에 기록함</br>(예: https://storage/{songId}_pose.json)</br></br><h2>3. 동작 유사도 비교 및 피드백</h2></br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/15156b6a737b6522ad238341eb82ac0d46d58dbe/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A43%EB%8F%99%EC%9E%91%20%EC%9C%A0%EC%82%AC%EB%8F%84%20%EB%B9%84%EA%B5%90%20%EB%B0%8F%20%ED%94%BC%EB%93%9C%EB%B0%B1.png" width="1100" height="600"></br>**■ 기능 개요**</br>동작 유사도 비교 및 피드백 기능은 사용자의 실시간 포즈 데이터를 기준 안무(전문가)의 포즈 데이터와 비교하여,프레임 단위 유사도 점수와 등급을 산출하고, 낮은 점수 구간에 대해 Gemini 기반 동작 피드백을 제공하는 기능입니다. 실시간 분석, 정량적 비교, 사용자 맞춤 피드백 생성이라는 세 요소가 결합된 복합적 기능입니다.</br></br>**■ 구성 요소**</br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/6f465f931bd04417cc224ef59856f744b6dff61d/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A43%EB%8F%99%EC%9E%91%20%EC%9C%A0%EC%82%AC%EB%8F%84%20%EB%B9%84%EA%B5%90%20%EB%B0%8F%20%ED%94%BC%EB%93%9C%EB%B0%B1_%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C.png" width="400" height=300></br></br>**■ 시퀀스 흐름**</br>**루프 구조 (2\~8): 실시간 프레임 캡처 및 분석 반복**</br>(1) 시스템이 기준 안무 데이터를 로드 (전문가 포즈 시퀀스 불러오기)</br>(2) 사용자가 카메라 앞에서 동작 시작 (→ 실시간 루프 진입)</br>(2.1) 클라이언트가 프레임 캡처</br>(3) POST /pose 요청으로 서버에 프레임 전송</br>(4\~5) 시스템이 프레임 번호를 계산하고 MediaPipe에 pose 추정 요청</br>(5.1) MediaPipe가 관절 위치 추정 후 반환</br>(6\~7) 시스템이 전문가 포즈와 사용자 포즈를 비교</br>&nbsp;&nbsp;&nbsp;&nbsp;→ DTW, 관절 각도, 위치 오차 기반 유사도 점수 계산</br>&nbsp;&nbsp;&nbsp;&nbsp;→ 점수에 따른 등급 결정</br>(8.1\~8.3) 점수와 등급을 사용자 클라이언트에 실시간 반환 → 화면에 출력</br></br>**동작 종료 및 하위 구간 분석 (9\~10)** </br>(9) 사용자가 연습을 종료함</br>(10\~10.4) 사용자 클라이언트가 유사도 점수 하위 5개 구간 요청</br>&nbsp;&nbsp;&nbsp;&nbsp;→ 시스템이 DB에서 해당 구간의 프레임 번호 조회</br>&nbsp;&nbsp;&nbsp;&nbsp;→ 각 프레임 번호에 해당하는 사용자 및 전문가 이미지 요청</br></br>**이미지 비교 및 피드백 생성 (11\~13)** </br>(11\~12) 클라이언트는 점수 하위 구간의 사용자 프레임을 녹화 영상에서 잘라내어 저장</br>&nbsp;&nbsp;&nbsp;&nbsp;→ 해당 프레임 번호에 맞는 사용자/전문가 이미지를 매핑</br>(13) 시스템이 각 프레임에 대해 사용자 vs 전문가 이미지 비교를 Gemini API에 요청</br>&nbsp;&nbsp;&nbsp;&nbsp;→ 요청에는 "프레임 번호, 사용자 이미지, 기준 이미지"가 포함됨</br>(13.1\~13.4) Gemini API가 동작 피드백 생성 후 반환</br>&nbsp;&nbsp;&nbsp;&nbsp;→ 시스템은 응답을 DB에 저장하고 사용자에게 피드백 전달</br>(13.5) 사용자 화면에 텍스트 피드백 출력</br></br><h2>4. 실루엣 마스크 생성 및 시각화</h2></br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/15156b6a737b6522ad238341eb82ac0d46d58dbe/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A44%EC%8B%A4%EB%A3%A8%EC%97%A3%20%EB%A7%88%EC%8A%A4%ED%81%AC%20%EC%83%9D%EC%84%B1%20%EB%B0%8F%20%EC%8B%9C%EA%B0%81%ED%99%94.png" width="700" height="400"></br>**■ 기능 개요**</br>실루엣 마스크 생성 및 시각화 기능은 전문가 안무 영상으로부터 프레임을 추출한 뒤, 각 프레임에서 인물(전문가)을 SAM2 엔진을 통해 분리(segmentation)하고, 이 마스크 정보를 기반으로 윤곽선이 강조된 실루엣 가이드를 생성해, 시각적으로 보조 가능한 네온 오버레이 영상으로 가공·저장하는 기능입니다. 이 영상은 사용자의 실시간 연습 시 동작 가이드용 오버레이 영상으로 활용됩니다.</br></br>**■ 구성 요소**</br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/69dc1e1e062316ab63997f10219fcf4b31d882ce/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A44%EC%8B%A4%EB%A3%A8%EC%97%A3%20%EB%A7%88%EC%8A%A4%ED%81%AC%20%EC%83%9D%EC%84%B1%20%EB%B0%8F%20%EC%8B%9C%EA%B0%81%ED%99%94_%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C.png" width="400" height=200></br></br>**■ 시퀀스 흐름**</br>① 관리자 → 관리자: 전문가 안무 영상 전처리</br>- 영상에서 프레임 이미지 시퀀스를 추출</br>(예: 30fps 기준 1초당 30장 이미지 → 총 수천 장의 프레임)</br></br>② 관리자 → SAM2 엔진: 프레임 시퀀스 업로드</br>- 추출한 이미지들을 SAM2 엔진으로 일괄 업로드</br></br>③ SAM2 엔진: Segmentation 처리 및 마스크 추출</br>- 각 프레임에 대해 인물(전문가)의 경계선을 정확하게 추출 (Mask 생성)</br>- SAM2 (Segment Anything Model v2)는 마우스 클릭이나 prompt 없이도 인물 자동 분리를 수행</br></br>③.1 SAM2 → 관리자: 마스크 정보 전달</br>- 각 프레임마다 마스크 배열 반환 (numpy array)</br>- 이 데이터는 영상 시각화에 사용될 기반 정보</br></br>④ 관리자: 마스크를 활용한 실루엣 시각화</br>- 마스크 이미지에 색상 및 외곽선 효과를 적용</br> (OpenCV의 cv2.drawContours, cv2.findContours 활용)</br>- 네온 효과, 외곽선 흐림 처리 등을 통해 실루엣 가이드를 시각적으로 강화</br></br>⑤ 관리자: 프레임별 실루엣 시퀀스를 하나의 영상으로 합성</br>- 시각화된 프레임들을 영상으로 인코딩 (cv2.VideoWriter 사용)</br>- 최종적으로 하나의 .mp4 영상 파일 생성</br></br>⑥ 관리자 → Storage: 실루엣 영상 업로드</br>- 최종 실루엣 영상을 스토리지에 업로드</br>- 이 파일은 사용자 연습 중 오버레이 재생용으로 활용됨</br></br>⑦ 관리자 → DB: 실루엣 영상 경로 저장</br>- 스토리지에서 응답받은 URL 또는 파일 경로를 DB에 저장</br></br><h2>5. 챌린지 모드 실행</h2></br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/15156b6a737b6522ad238341eb82ac0d46d58dbe/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A45%EC%B1%8C%EB%A6%B0%EC%A7%80%20%EB%AA%A8%EB%93%9C%20%EC%8B%A4%ED%96%89.png" width="700" height="700"></br>**■ 기능 개요**</br>챌린지 모드 실행 기능은 관리자가 Unity를 통해 가상 아바타와 안무 데이터를 결합하여 아바타 영상을 사전에 생성하고, 사용자가 챌린지 모드에서 노래를 선택하면 해당 아바타 안무 영상을 불러와 화면에 출력하는 흐름을 포함합니다. 이 과정은 관리자-생성 파이프라인과 사용자-실행 파이프라인으로 나뉘며, 각각에 대해 분리된 시퀀스를 갖습니다.</br></br>**■ 구성 요소**</br><img src="https://github.com/sohee6989/Capstone_Capjjang/blob/69dc1e1e062316ab63997f10219fcf4b31d882ce/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/%EC%9C%A0%EC%8A%A4%EC%BC%80%EC%9D%B4%EC%8A%A45%EC%B1%8C%EB%A6%B0%EC%A7%80%20%EB%AA%A8%EB%93%9C%20%EC%8B%A4%ED%96%89_%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C.png" width="400" height=320></br></br>**■ 시퀀스 흐름**</br>**관리자 시퀀스: 아바타 영상 생성 및 등록**</br>① 관리자 → Unity: 안무 데이터 전달</br>- 관리자가 챌린지에 사용할 안무 데이터를 Unity에 입력</br>- Unity 프로젝트는 3D 아바타 모델과 해당 안무 데이터를 시각적으로 결합할 준비를 함</br></br>①.1 Unity 내부: 아바타와 안무 데이터 결합</br>- Skeleton 기반 리깅 및 애니메이션 매핑이 이루어짐</br>- 아바타가 안무를 추는 형태로 씬이 구성됨</br></br>①.2 Unity → 관리자: 아바타 영상 생성 완료</br>- Unity에서 영상을 .mp4로 렌더링하여 관리자에게 반환</br>- 이는 영상 기반 가이드로 사용됨</br></br>② 관리자 → Storage: 생성된 아바타 영상 업로드</br>- 영상은 AWS S3 Storage에 업로드되어 사용자 요청 시 접근 가능하도록 함</br></br>②.1 시스템 → DB: 아바타 영상 경로 저장</br>- 업로드된 영상의 URL 또는 내부 경로를 DB에 메타데이터로 저장</br>- 이후 사용자 실행 시 시스템이 해당 경로를 참조하게 됨</br></br>**사용자 시퀀스: 챌린지 모드 실행**</br>③ 사용자 → 사용자 클라이언트: 노래 선택</br>- 사용자는 챌린지 모드에서 노래를 선택함으로써 아바타 영상 실행을 트리거</br></br>④ 사용자 클라이언트 → 시스템: 노래 정보 요청</br>- 해당 노래에 대한 아바타 영상이 존재하는지 확인</br></br>④.1 시스템 → DB: 노래 정보 및 영상 경로 조회</br></br>④.2~3 DB → 시스템 → 클라이언트: 노래 및 영상 경로 반환</br></br>⑤ 사용자 클라이언트 → Storage: 영상 스트리밍 요청</br>- 반환된 영상 경로(URL)를 기반으로 Storage에 접근하여 아바타 영상을 불러옴</br></br>⑤.1 클라이언트: 화면에 아바타 영상 출력</br>- Unity로 제작된 아바타가 안무를 추는 영상을 실시간으로 사용자 화면에 오버레이</br>- 챌린지 모드의 시각적 가이드 역할을 수행</br></br>[기능별 상세 요구사항 링크](https://weak-cowl-8b6.notion.site/165e4443a480808ab8dbdaaa2ace42d3?v=d3ca2b8d9f6c4f3e85bbf90a5abe1684)</br> |
| (2) 전체 시스템 구성 |  ![](https://github.com/sohee6989/Capstone_Capjjang/blob/main/2%EC%B0%A8%20%EB%B3%B4%EA%B3%A0%EC%84%9C/SWarchitecture.png)
</br></br> **1. 사용자 포즈 인식** </br> 사용자가 모드와 노래를 선택하면 AWS RDS에 노래 관련 데이터의 S3 경로가 요청됩니다.</br> 이후 사용자가 카메라를 실행하면, Frontend에서 OpenCV를 기반으로 사용자 동작이 실시간으로 감지되고, 영상 데이터를 Backend로 전송합니다.</br> Backend는 MediaPipe를 통해 영상에서 사용자의 신체 주요 키포인트(x, y, z 좌표)를 추출합니다.</br> 추출된 키포인트 데이터는 AWS RDS에 저장되어 활용됩니다.
</br></br> **2. 전문가 안무 데이터 로딩** </br> 사용자가 선택한 곡에 맞춰 Backend에서 전문가 안무 영상과 포즈 JSON 데이터를 로드하면, OpenCV를 통해 전문 안무가의 동작이 감지됩니다.</br> Backend는 MediaPipe를 활용해 안무 영상에서 전문가의 주요 키포인트(x, y, z 좌표)를 추출하여 AWS S3에 저장한다. S3에는 키포인트 데이터뿐 아니라 노래 데이터, 실루엣 영상, 사용자 영상도 함께 저장됩니다.</br> 이후 사용자 데이터와의 비교 분석 및 피드백 생성에 활용됩니다.
</br></br> **3. 동작 유사도 비교 및 피드백** </br> 정확도 모드에서 사용자가 곡을 선택하면, Frontend는 AWS RDS에서 해당 곡의 S3 경로를 요청합니다. 이 경로를 통해 AWS S3에서 노래 관련 데이터를 불러오고, 사용자 화면에서 노래와 함께 카메라가 실행됩니다.</br> 카메라가 실행되면 OpenCV를 통해 사용자 동작이 실시간으로 감지됩니다. Backend는 S3에서 로드한 전문가 안무 키포인트와, 사용자의 키포인트를 MediaPipe 기반으로 하여 비교합니다. 사용자 영상의 구간별 유사도를 계산하고, 계산된 점수에 따라 best, good, bad 등으로 등급을 매긴 후 다시 Frontend로 전송합니다. 등급은 텍스트 형태로 사용자 화면에 실시간으로 출력됩니다.</br> 연습이 끝나면 어플리케이션에서 사용자 점수 하위 5개 구간을 요청하고, Backend가 DB에서 해당 프레임 번호 및 사용자와 전문가 이미지를 조회합니다. 서버에서 이미지들을 보내면, 모델이 프레임별 한 줄 피드백을 생성해 반환합니다. Backend는 피드백을 DB에 저장한 뒤 피드백을 함께 Frontend에 전달하고, 사용자는 화면에서 구간별 맞춤 코칭을 확인합니다.
</br></br> **4. 실루엣 마스크 생성 및 시각화** </br> 사용자가 모드와 노래를 선택하면, Frontend는 AWS RDS에서 곡에 대한 S3 경로를 요청하여 노래 관련 데이터를 불러오고,사용자 화면에서 노래와 함께 카메라가 실행됩니다.</br> OpenCV를 이용해 안무 영상에서 안무가의 마스크 외곽선을 감지하고 실루엣을 시각적으로 강조하고, SAM2를 이용해 안무 실루엣 가이드 영상을 제작합니다. 완성된 영상은 AWS S3에 저장됩니다.</br> Backend는 AWS S3로부터 실루엣 영상의 URL 정보를 Frontend로 전달하여 사용자의 카메라 화면에 실루엣 가이드가 실시간으로 출력될 수 있도록 합니다.
</br></br> **5. 챌린지 모드 실행** </br> 챌린지 모드에서 사용자가 곡을 선택하면, Frontend는 AWS RDS에서 해당 곡의 S3 경로를 요청합니다. 이 경로를 통해 AWS S3에서 노래 관련 데이터를 불러오고, 사용자 카메라가 실행됩니다.</br> Unity는 K-pop 안무 키포인트 데이터베이스에서 동작 데이터를 불러와 아바타에 적용하고, 이를 이용해서 아바타 영상을 제작합니다. 제작된 아바타 영상은 AWS S3에 저장되며, Frontend에서 해당 영상을 로드하여 사용자 화면에 출력합니다. 그리고 사용자 화면에서 아바타 움직임을 확인할 수 있습니다.|
| (3) 주요엔진 및 기능 설계 | **1. Client Module (Android App)** <br><li> 역할: 사용자가 앱을 통해 촬영한 실시간 안무 영상을 서버로 전송하고, 서버가 반환한 점수·텍스트 피드백·실루엣 영상을 받아 즉시 화면에 시각적으로 표시합니다. 또한 곡 선택·모드 선택(연습 / 정확도 / 챌린지)·로그인 기능을 제공합니다. <br><li> 기술: <br>  <li> Kotlin + Android Studio로 개발<br>  <li>CameraX로 실시간 촬영<br>  <li>Retrofit2에서 POST /upload로 영상 스트림을 multipart‑form 데이터로 전송하고, GET /result로 분석 점수(JSON)·피드백·S3 영상 URL 수신
 <br><li> 기능 상세: <br>  <li> 실시간 영상 촬영 및 전송 <br>  <li> REST API를 통해 EC2 서버로 분석 요청 전송  <br>  <li> 분석 결과 수신 및 UI 표시 <br><li> 보안: JWT / OAuth  Bearer Token을 Authorization 헤더에 첨부해 요청 인증 
 <br><br>**2. IntelliJ + JAR Build** <br><li> 역할: 서버 애플리케이션(Spring Boot)을 IDE에서 개발한 뒤 JAR 파일로 패키징하고, 자동 스크립트로 AWS EC2 인스턴스에 전송·배포해 최신 버전을 즉시 서비스에 반영합니다. <br><li> 기술: <br>  <li> IntelliJ IDEA Ultimate + Gradle 빌드 시스템
<br>  <li>Java 21 기반 Spring Boot 애플리케이션<br>  <li>배포 스크립트: Bash + scp/rsync (SSH Key 방식) / Docker Compose로 컨테이너 구동
 <br><li> 기능 상세: <br>  <li> 로컬에서 빌드와 테스트를 완료한 뒤  bootJar를 실행해 최종 JAR 파일을 생성 <br>  <li> 생성된 JAR 파일을 scp(또는 rsync)로 EC2 서버의 /deploy 디렉터리에 업로드<br>  <li> 원격 서버에서 기존 app 컨테이너를 중지·삭제한 후, 새 JAR을 사용해 Docker run 명령으로 컨테이너를 재시작함 
 <br><br>**3. Amazon EC2 + Docker 기반** <br>**▷ 3-1. Spring Boot** <br><li> 역할:  클라이언트의 API 요청을 받아 처리하며, 내부 모듈(Flask, Redis, S3, MySQL)을 조정합니다. <br><li> 기술: Java 21 + Spring Boot 3.4, Docker 컨테이너로 실행 <br><li> 기능 상세: <br>  <li> /upload, /result/{id} 등의 REST API를 제공해 영상 업로드와 결과 조회를 담당<br>  <li> 요청별로 Redis에 상태 값을 기록하고 조회<br>  <li> Flask 분석 서버에 gRPC/HTTP로 영상을 전달하고, 분석 결과를 받아 사용자별로 정리<br>  <li> 완성된 결과 영상을 S3에 저장하고, 관련 메타데이터를 MySQL(RDS)에 기록
 <br><br>**▷3-2. Redis** <br><li> 역할: Spring Boot와 Flask 사이의 작업 큐이자 캐시로 동작합니다. <br><li> 기능 상세: <br>  <li> 각 요청의 requestId와 진행 상태(PENDING, PROCESSING, DONE)를 키‑값 형태로 유지<br>  <li> 중간 점수·프레임 번호 등 실시간 데이터를 잠시 저장해 빠르게 읽어옴
 <br><br>**▷ 3-3. Flask** (Python) <br><li> 역할: 업로드된 영상을 분석해 관절 좌표 추출과 포즈 점수 계산, 실루엣 생성을 수행합니다. <br><li> 기술:  Python 3.11, Flask, OpenCV와 MediaPipe로 실행 <br><li> 기능 상세: <br>  <li> 영상에서 프레임을 추출하고 MediaPipe로 33개 관절 좌표를 얻음<br>  <li> 안무 동작에서의 관절 간 거리와 각도 차이를 계산해 포즈 정확도 점수를 산출<br>  <li> OpenCV로 인물 영역을 바이너리 마스크하여 실루엣 영상으로 변환<br>  <li> 결과(점수 JSON, 실루엣 영상)를 S3에 저장하고 Redis에 상태를 DONE으로 갱신 
 <br><br>**4. Amazon S3**<br><li> 역할: 원본 안무 영상·실루엣 영상·사용자 촬영 영상을 안전하게 보관하고, 앱·서버가 필요할 때 URL로 접근할 수 있게 하는 클라우드 객체 저장소입니다.<br><li> 기능 상세: <br>  <li> Flask가 생성한 실루엣 MP4·PNG를 dance silhouette 버킷에 저장함<br>  <li> 앱 내에서 사용자가 촬영한 원본 영상을 직접 업로드함<br>  <li> 업로드 완료 시 Pre‑Signed URL을 생성해 Spring Boot 서버를 거쳐 DB에 기록하고 클라이언트로 전달함 
 <br><br>**5. Meta SAM2**<br><li> 역할: K‑pop 안무 영상에서 고정밀 외곽선 마스크를 추출하여 실루엣 가이드를 만듭니다.<br><li> 기술: Meta SAM2 모델(T4 GPU) REST API<br><li> 기능 상세: <br>  <li> Spring Boot가 안무 영상 프레임을 SAM2의 API에 전송<br>  <li> SAM2가 사람 영역에 대해 마스크를 생성 후 로컬 PNG 세트로 반환<br>  <li> Flask가 마스크를 합성해 실루엣 영상을 만들고, S3에 저장
 <br><br>**6. Amazon RDS (MySQL)** <br><li> 곡 정보, 세션 로그, 점수·피드백, S3 경로 등 메타데이터를 저장·조회하는 관계형 DB입니다.<br><li> DB Connection: Spring Boot가 JPA로 연결<br><li> 기능 상세: <br>  <li> 곡 정보를 song 테이블에 저장함 (곡 ID, 제목, 가수명 등)<br>  <li> 요청 세션을 session 테이블에 기록함<br>  <li> 구간별 점수와 AI 피드백을 저장함<br>  <li> Spring Boot가 트랜잭션으로 INSERT/UPDATE를 수행하고, Flask 결과 수신 시 상태를 DONE으로 갱신함 
 <br><br>**7. Google Gemini Vision AI**<br><li> 역할: 사용자가 연습을 종료하면, 유사도 점수 하위 3-4개 구간의 사용자와 전문가의 이미지(혹은 영상)을 입력받아, gemini-2.0-flash-001모델로 비교해 한 줄 피드백을 생성한 후 Spring Boot에서 어플리케이션으로 전달합니다. <br><li> 기술: <br>  <li> Google Gemini Vision API (gemini-2.0-flash-001; Gemini Flash 2.0)<br>  <li> 호출 방식: Flask → Gemini REST POST, 응답 비동기(≤ 800 ms 목표) <br><li> 기능 상세: <br>  <li> 연습 종료 시 앱이 /low-score API로 점수 하위 3-4구간을 요청함<br>  <li> Spring Boot가 RDS에서 하위 구간 프레임 번호와 이미지 경로를 조회함<br>  <li> Flask가 사용자·전문가 PNG를 해당 프레임 번호로 추출·매핑함<br>  <li> 이미지 쌍을 Gemini Flash로 배치 전송해 한 줄 한국어 피드백을 요청함<br>  <li> Flash 응답을 수신해 {frameNo, feedbackKo}를 RDS에 저장함<br>  <li> Spring Boot가 점수·피드백을 /result 응답에 포함해 앱으로 전송함<br>  <li> 앱이 화면에 구간별 피드백 문구를 실시간 표시함<br><li> 현재 단계: Gemini Flash 연동·성능 검증은 미진 상태이며, 프롬프트 설계만 완료됨
 <br><br>**■ 전체 프로세스 흐름 요약:** <br>User (App) → 곡·모드 선택 후 API 요청 전송<br>Spring Boot → RDS에서 곡 메타데이터·전문가 키포인트 S3 경로를 조회하고 Redis에 작업을 등록<br>Flask + MediaPipe → 영상 분석 → 프레임을 분석해 33개 관절 좌표를 추출하고 사용자의 구간별 점수를 계산<br>Gemini API → 점수가 낮은 하위 3구간 클립을 전문가 클립과 비교해 간단 피드백 생성<br>실루엣 MP4 영상·PNG 및 결과 영상을 생성 → Amazon S3 버킷에 업로드<br>로그 → 점수·피드백·S3 URL을 Amazon RDS 기록하고 Redis 상태를 DONE으로 갱신<br>App → 점수·피드백·실루엣 URL을 받아 평가 결과를 확인


|
| (4) 주요 기능의 구현 | **1. 동작 유사도 분석 기능 (MediaPipe 기반)** <br>사용자가 안무를 연습할 때 자신의 동작이 전문가의 동작과 얼마나 유사한지를 스스로 판단하기 어렵다는 문제를 해결하기 위해 MediaPipe Pose 모델을 활용한 실시간 유사도 분석 기능을 구현했습니다. 이를 통해 사용자는 실시간으로 피드백을 받아 안무를 효율적으로 교정할 수 있습니다.<br><li>사용자의 동작을 정확하게 분석하기 위해 MediaPipe의 Pose 모듈을 이용해 사용자 영상에서 인체의 주요 관절 위치(어깨, 팔꿈치, 손목, 엉덩이, 무릎, 발목 등)의 x, y, z 좌표를 프레임 단위로 추출합니다.<br><li>추출된 관절 좌표는 전문가의 기준 데이터와 비교되며 유클리드 거리 및 관절 각도 차이를 기반으로 유사도를 수치화합니다.<br><li>실시간으로 계산된 점수는 Best / Good / Bad 등급으로 구분되며 해당 등급에 따른 피드백은 텍스트 형태로 사용자 화면에 출력됩니다. <br><br> (실행 흐름 요약) <br>1. 사용자가 정확도 모드 및 곡 선택 <br>2. Frontend가 전문가 키포인트 데이터 로드 <br> 3. 카메라 실행, 사용자 동작을 OpenCV로 감지 <br> 4. Backend가 MediaPipe로 관절 좌표 추출 <br> 5. 전문가 데이터와 비교 -> 유사도 점수 계산 <br>6. 실시간 등급 + 피드백 텍스트 Frontend에 출력 <br><br>=> 이를 통해 사용자는 별도의 장비나 공간 제약 없이, 정확하고 직관적인 피드백을 통해 안무 실력을 향상시킬 수 있습니다.<br><br>**2. 맞춤형 피드백 기능 (Gemini)** <br> 정확도 모드에서 사용자가 안무 연습을 완료하면 시스템은 유사도 점수 하위 구간을 자동으로 식별한 후 해당 구간의 이미지를 Gemini에 전달하여 사용자에게 맞춤형 한 줄 요약 피드백을 생성하는 기능입니다. <br><li>서버가 이미지를 정리하고 Gemini에게 전송하면 Gemini는 두 이미지 간의 자세 차이를 인식하고 피드백을 반환합니다.<br><li>해당 피드백은 서버를 통해 앱으로 전달됩니다.<br><li>사용자는 구간별 맞춤 피드백을 확인하고 부족한 동작을 집중적으로 개선할 수 있습니다. <br><br>(실행 흐름 요약)<br>1. 연습 종료 후 점수 하위 3~5개 구간 추적 <br>2. Spring Boot가 AWS RDS에서 하위 구간의 프레임 번호 및 이미지 경로 조회 <br> 3. 서버에서 해당 프레임을 추출하여 PNG 이미지로 저장 <br> 4. Gemini로 이미지 전송 <br> 5. Gemini가 프레임별 1줄 한국어 피드백 반환 <br> 6. Frontend는 각 구간의 피드백을 출력 <br><br>=> 이를 통해 사용자는 단순 점수 외에도 구체적인 동작 교정 방향을 AI로부터 안내받을 수 있으며 개선이 필요한 구간을 정확히 인지하고 반복 학습할 수 있습니다.<br>현재 단계: Gemini Flash 연동·성능 검증은 미진 상태이며, 프롬프트 설계만 완료되었습니다.<br><br>**3. 안무 실루엣 추출 및 시각화 기능 (SAM2 + OpenCV)**<br>전문가의 안무 영상을 바탕으로 사용자가 정확한 동작을 인지하고 따라할 수 있도록 실루엣 가이드 영상을 생성한 후, 화면에 오버레이하는 기능입니다.<br><li>SAM2를 이용해 안무가 영상에서 객체(사람)를 자동으로 인식하고, 전체 프레임에서 해당 객체를 추적하며 세그멘테이션 마스크를 생성합니다.<br><li>OpenCV의 findContours와 drawContours 기능을 이용해 윤곽선을 시각화하며, GaussianBlur를 활용해 네온 효과를 적용함으로써 동작 흐름을 더 명확하게 표현합니다.<br><li>완성된 실루엣 가이드 영상은 AWS S3에 저장합니다.<br><li>사용자가 모드 및 곡을 선택하면 Backend가 해당 영상 경로를 AWS RDS에서 조회한 뒤 실루엣 영상을 사용자의 카메라 위에 오버레이 형태로 출력합니다.<br><br> (실행 흐름 요약) <br>1. 사용자가 연습/정확도 모드에서 노래 선택 <br> 2. Frontend → RDS로 곡 관련 실루엣 영상 URL 요청 <br> 3. Backend가 S3에서 해당 영상 반환 → Frontend로 전달 <br> 4. 사용자의 카메라 영상 위에 실루엣 영상이 오버레이되어 표시<br><br>=>  이를 통해 사용자는 거울 없이도 안무가의 자세를 따라 하기 쉽고, 직관적으로 동작을 인식할 수 있습니다.<br><br>**4. 챌린지 모드 기능 (Unity 기반)**<br>사용자가 3D 아바타와 함께 춤을 추며 안무를 연습할 수 있도록 구성된 기능입니다. 반복 학습의 지루함을 해소하고, 몰입도 높은 학습 경험을 제공합니다.<br><li>Unity는 K-pop 안무 키포인트 데이터베이스로부터 동작 데이터를 불러와 3D 아바타에 모션을 적용합니다. 이를 통해 안무에 맞춰 아바타가 춤을 추는 영상을 생성합니다. <br><li>생성된 아바타 영상은 AWS S3에 저장합니다.<br><li>사용자가 챌린지 모드에서 곡을 선택하면 S3에서 경로를 조회하고 해당 아바타 영상을 로드하여 사용자 화면에 출력합니다.<br><li> 카메라 위에 아바타 영상이 오버레이되어 실시간으로 아바타의 동작을 함께 확인합니다.<br><br> (실행 흐름 요약)<br>1. 사용자가 챌린지 모드에서 곡 선택<br>2. Frontend → RDS로 S3 영상 경로 요청<br>3. Backend에서 아바타 영상 로딩<br>4. 아바타 영상이 사용자 화면에 실시간 출력<br>5. 사용자는 아바타와 함께 안무 수행 <br><br>=> 이를 통해 사용자가 캐릭터와 함께 안무를 추며 단순 반복 학습의 지루함을 해소시키고, 상호작용과 성취감을 제공하며 몰입도 높은 학습 경험을 유도합니다.| (5) 기타 ||<br>
